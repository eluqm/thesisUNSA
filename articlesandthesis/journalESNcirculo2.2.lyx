#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass elsarticle
\begin_preamble
\usepackage{color}
\usepackage{algorithm,algpseudocode}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\definecolor{maroon}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{orangered}{RGB}{239,134,64}
\definecolor{orchid}{RGB}{218,112,214}
\definecolor{violet}{RGB}{238,130,238}
\tnotetext[t1]{This document is a collaborative effort.}
%\tnotetext[t2]{The second title footnote which is a longer
%longer than the first one and with an intention to fill
%in up more than one line while formatting.}
%\author[rvt]{E.F.~Luque\corref{cor1}\fnref{fn1}}
%\ead{edluquem@usp.br.com}
\author[rvt,rvt2]{E.F.~Luque}
\ead{edluquem@usp.br.com}
\author[focal]{D.L.~Rubin}
\ead{dlrubin@stanford.edu}
\author[rvt]{D.A.~Moreira}
\ead{dilvan@gmail.com}


%\ead[url]{http://www.elsevier.com}
%\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Principal corresponding author}
%\fntext[fn1]{This is the specimen author footnote.}
%\fntext[fn2]{Another author footnote, but a little more
%longer.}
%\fntext[fn3]{Yet another author footnote. Indeed, you can have
%any number of author footnotes.}

\address[rvt]{Department of Computer Science\\ University of São Paulo\\ São Carlos, Brazil}
\address[rvt2]{Department of Computer Science\\ National University of San Agustin\\ Arequipa, Peru}
\address[focal]{Department of Radiology\\ Stanford University\\ Stanford, USA\\}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{frontmatter}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{abstract}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

The stochastic streamflow models (SSMS) are time series models for precise
 prediction of hydrological data.
 They could generate ensembles of synthetic time series traces useful for
 hydrologic risk management.
 Nowadays, deep learning networks get many considerations in time series
 prediction.
 However, despite their theoretical benefits, they fail due to their architectur
e, defects of the backpropagation method, such as  slow convergence and
 the vanishing gradient problem.
 In order to cover these requirements, we propose a new stochastic model
 applied in problems that involve phenomena of stochastic behavior and periodic
 characteristics.
 In the new model two components were used, the first one, a type of recurrent
 neural network embedding an echo-state (ESN) learning mechanisn instead
 of conventional back-propagation method, an interesting feature of ESN
 is that from certain algebraic properties, training only the output of
 the network is often sufficient to achieve excellent performance in practical
 applications.
 The last part consists of the uncertainty associated with stationary processes,
 the model is finally called stochastic streamflow models ESN (SSNESN).
 This model was calibrated with time series of monthly discharge data from
  different river basins of MOPEX data set.
 We interpret our expetimental findings by comparison with two feedforward
 neural networks and the traditional Thomas–Fiering model.
 The results show that the SSNESN can achieve a significant enhancement
 in the prediction performance, learning speed, and short-term memory capacity,
 with interesting potential in the context of hydrometeorological resources.
 This model, along with their simplicity and ease of training, can be considered
 a first attempt that applies the echo state network methodology to stochastic
 process.
 
\end_layout

\begin_layout Plain Layout


\backslash
end{abstract}
\end_layout

\begin_layout Plain Layout


\backslash
begin{keyword}
\end_layout

\begin_layout Plain Layout

ESN, Recurrent, Stochastic process, Neural Network, streamflow.s
\end_layout

\begin_layout Plain Layout


\backslash
end{keyword}
\end_layout

\begin_layout Plain Layout


\backslash
end{frontmatter}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In probability theory an stochastic process is defined as a set of models
 that allow the study of problems with random components.
 Observing a phenomenon with random characteristics for a period of time,
 it is possible to obtain a trajectory of this observed process.
 When carrying out the same observation in a different period of time it
 is possible to obtain another trajectory, different from the first one.
 Thus, stochastic process corresponds to the set of all possible trajectories
 that can be observed of this phenomenon.
 Each trajectory are called a time series.
 Therefore, time series is considered one realization of stochastic process.
\end_layout

\begin_layout Standard
Natural phenomena such as precipitation and streamflow discharge have nonlinear,
 complex and chaotic characteristics.
 In order to model the behavior of these phenomena, initially linear approximati
on was used 
\begin_inset CommandInset citation
LatexCommand cite
key "BOXANDJenKys"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "LUNA2006"

\end_inset

.
 Afterwards, were developed methods using self-correcting models such as
 the PAR(p) model 
\begin_inset CommandInset citation
LatexCommand cite
key "maceira2,lcdcampos"

\end_inset

.
 However, these models are statistical and linear, which means that their
 application in time series of chaotic behavior such as hydrometeorological
 series, cannot capture real characteristics of this time series being sometimes
 inadequate 
\begin_inset CommandInset citation
LatexCommand cite
key "Raman95multivariatemodelling"

\end_inset

.
\end_layout

\begin_layout Standard
Currently, among the approaches that attempt to model complex non-linear
 behavior that can be adequate to deal with such problems, we have the deep
 learning approach 
\begin_inset CommandInset citation
LatexCommand cite
key "YU2015308"

\end_inset

, this approach exerts fascination on researchers due their hierarchical
 representations from data.
 In fact, this approach have been widely used in most research and applications
 in forecasting models from simplest Artificial Neural Networks (ANN) feedforwar
d to the most complex architecture based on many layers, each of which be
 expressed by feature detector units.
 In general, ANNs feedforward have been widely used in most research and
 applications in forecasting models in contrast to Recurrent Neural Networks(RNN
)
\begin_inset CommandInset citation
LatexCommand cite
key "hess-17-253-2013"

\end_inset

.
 The RNN's are able to represent dynamic non-linear maps commonly found
 in time series forecasting tasks
\begin_inset CommandInset citation
LatexCommand cite
key "lcdcampos"

\end_inset

.Studies about the performance in forecasting, demonstrate that RNN are better
 than their peers ANN, in virtually all tests
\begin_inset CommandInset citation
LatexCommand cite
key "6327793"

\end_inset

.
 However, the main reason to prefer to use the ANN feedforward higher than
 recurrent neural networks is that the last generates greater complexity
 in the neural network training process.
 Massive iterative computation results in a slow convergence rate, as well
 as the backpropagation algorithm based on gradient descent can be trapped
 into a local optimum
\begin_inset CommandInset citation
LatexCommand cite
key "SUN201717"

\end_inset

.
 All it is added to the complexity that uncertainty analysis and stochastic
 simulation requires.
 
\end_layout

\begin_layout Standard
This motivated the development of a stochastic process model using Recurrent
 Artificial Neural Networks in order to take advantage of its aforementioned
 characteristics.
 this was possible using an approach called Reservoir Computing(
\series bold
RC
\series default
)
\begin_inset CommandInset citation
LatexCommand cite
key "LukoseviciusJaeger09"

\end_inset

.
 Reservoir Computing is a training approach attractive as simple and fast
 compared to other approaches used in traditional recurrent ANN, all in
 order to reduce complexity, and leverage its proven ability to represents
 the characteristics of time series.
 Our model also is composed for the nondeterministic component that represents
 the white noise with a normal distribution, in order to take into account
 the uncertainty that normally affects natural processes 
\begin_inset CommandInset citation
LatexCommand cite
key "Awchi"

\end_inset

.
 Therefore, our model could be considered a new proposal in the literature.
 Finally, as a case study, it was chosen for apply this model in the well-known
 Model Parameter Estimation Experiment(
\series bold
MOPEX
\series default
) data set
\begin_inset CommandInset citation
LatexCommand cite
key "Duan20063"

\end_inset

.
 
\end_layout

\begin_layout Section
History
\end_layout

\begin_layout Standard
Initial auto-regressive models, as well as models based on Box & Jenkis
 
\begin_inset CommandInset citation
LatexCommand cite
key "BOXANDJenKys"

\end_inset

 methodology were used in forecasting problems, for example in 
\begin_inset CommandInset citation
LatexCommand cite
key "Peng2010"

\end_inset

, the author shows that there is no evidence that multivariate AR(1) models
 are inadequate as predictors.
 Studies such as 
\begin_inset CommandInset citation
LatexCommand cite
key "JAWR:JAWR70A"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Thomas"

\end_inset

 describe mathematical models, which can reproduce special characteristics
 such periodicity, considering the effects of linear correlation.
 All these studies propose, the natural behavior of times series can be
 simulated by simple linear relationship with previous data.
 
\end_layout

\begin_layout Standard
The problem with previous models is that forecasting is naturally a dynamic
 task.
 For this reason, artificial intelligence methods have been appearing as
 alternatives, with good performance as predictors of time series.
 Works such as 
\begin_inset CommandInset citation
LatexCommand cite
key "WRCR:WRCR9735"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "WRCR:WRCR11667"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lcdcampos"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "hess-20-1405-2016"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "4371334"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Awchi"

\end_inset

, they used Artificial Neural Networks (ANN) as forecasting models.
 Recently, hybrid deep neural networks such as DNN 
\begin_inset CommandInset citation
LatexCommand cite
key "deng"

\end_inset

 and its variations 
\begin_inset CommandInset citation
LatexCommand cite
key "SHEN2015243,KUREMOTO201447"

\end_inset

 also show great potential for time series prediction.
 Although these proposals have shown favorable prediction performance, global
 weight "fine-tuning" in the logistic regression layer using the backpropagation
 algorithm, limits their further development in time series prediction.
\end_layout

\begin_layout Standard
Recurrent Neural Networks have show better forecasting ability than feedforward
 ones, due to their structure
\begin_inset CommandInset citation
LatexCommand cite
key "hess-17-253-2013"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "LukoseviciusJaeger09"

\end_inset

.
 This structure allows more parsimonious modeling of the dynamical time
 series properties.
 However, it was also demonstrated that, the recurrence structure could
 cause increased training complexity, and subsequently cause problems of
 convergence
\begin_inset CommandInset citation
LatexCommand cite
key "1"

\end_inset

.
 At the beginning of the 21st century, the concept of Reservoir Computing
 (RC) was introduced.
 
\end_layout

\begin_layout Standard
Reservoir Computing is a training approach that can be remarkably simpler
 and faster than those traditional applied in Recurrent ANN.
 Studies using RC in time series prediction were developed by 
\begin_inset CommandInset citation
LatexCommand cite
key "hess-17-253-2013"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "en81012228"

\end_inset

, the most popular reservoir computing model denominated 
\series bold
Echo State Networks (ESN)
\series default
 was used with a Bayesian regularization for forecast short-term energy
 production of small hydroelectric plants, the results indicate that the
 proposed model surpasses both, recurrent ANN feedforward and ESN in its
 simple version.
\end_layout

\begin_layout Standard
Studies such as 
\begin_inset CommandInset citation
LatexCommand cite
key "Coulibaly201076"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Abrahart201276"

\end_inset

 used ESN to forecast monthly water levels for the four Great Lakes of North
 America, the authors obtained good performance of ESN networks and attributed
 it to their highly non-linear and dynamic structure.
 Echo state networks were found to be valid alternatives to traditional
 recurrent ANN, in forecasted water inflow, for example in 
\begin_inset CommandInset citation
LatexCommand cite
key "4371334"

\end_inset

 the authors compare the performance of ESN with SONARX 
\begin_inset CommandInset citation
LatexCommand cite
key "4371335"

\end_inset

 networks, RBF networks and the ANFIS model 
\begin_inset CommandInset citation
LatexCommand cite
key "256541"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "hess-17-253-2013"

\end_inset

, forecasting ability from ESN model was evaluated in a huge variety of
 large-scale basins.
 The experiments are carried out by comparing three different ESN variations
 with two RNN feedforward models.
 In addition, several aspects of ESN design are investigated in order to
 optimize hydrologically relevant information.
 Only 
\begin_inset CommandInset citation
LatexCommand cite
key "en81012228"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "SUN201717"

\end_inset

, present a hybrid system as our model proposal, in order to take advantage
 that a hybrid system can provide us in forecasting task.
 
\end_layout

\begin_layout Standard
Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Recurrent-Neural-Networks"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Training-Algorithms"

\end_inset

 briefly reviews feedforward and traditional recurrent ANN models and their
 training methods, after which a short introduction to RC is given and ESN
 is described more detailed.
 Section Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Hidrometereological-data-generat"

\end_inset

 presents details about our proposal.
 The results of some experiments are presented and discussed in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Results-and-Discussion"

\end_inset

.
 Finally, conclusions are drawn in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Conclusions"

\end_inset

.
\end_layout

\begin_layout Section
Theoretical Background 
\end_layout

\begin_layout Standard
In this section, we briefly summarize the theoretical background of the
 considered models, namely RNN and ESN, which are the basis of the following
 SSNESN understanding.
\end_layout

\begin_layout Subsection
Recurrent Neural Networks 
\begin_inset CommandInset label
LatexCommand label
name "sec:Recurrent-Neural-Networks"

\end_inset


\end_layout

\begin_layout Standard
Recurrent Neural Networks (RNN) are a subclass of ANN characterized by cyclic
 graphs in its structure.
 These cycles accumulate previous activities and allow the network stores
 internal states.
 These internal states avoid needing to feed the network with the history
 of previous input and output as the Time-Delay Neural Network
\begin_inset CommandInset citation
LatexCommand cite
key "Kuna2015"

\end_inset

.
 And can use the input sequences in order to perform temporal tasks as forecasti
ng.
 The RNN output can be described by: 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Formula $Output_{t+1}\cong Forecasting\left(RNNstate,Input_{t},Output_{t}\right)$
\end_inset


\end_layout

\begin_layout Standard
The figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:elmanandwilliam"

\end_inset

 shows the two types of traditional recurrent ANN models, the Elman recurrent
 network 
\begin_inset CommandInset citation
LatexCommand cite
key "Elman90findingstructure"

\end_inset

 and the Willians-Zipser fully recurrent network
\begin_inset CommandInset citation
LatexCommand cite
key "Williams:1989:LAC:1351124.1351135"

\end_inset

.
 These neural networks have cyclic connections on their structure.
 For instance, the Elman network connects its input to all neurons, including
 output ones, hidden and output neurons are fully interconnected.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/RNNewilliamandelman.pdf
	lyxscale 30
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:elmanandwilliam"

\end_inset

(b) Elman recurrent Artificial Neural Network.
 a) William-zipser fully recurrent Artificial Neural Network.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Learning Algorithms
\begin_inset CommandInset label
LatexCommand label
name "sec:Training-Algorithms"

\end_inset


\end_layout

\begin_layout Standard
Basically, supervised learning means to adjust the network weight matrix
 
\begin_inset Formula $W$
\end_inset

 using the optimization algorithms, in order to minimize the output error.
 This is probably the most common approach used among the current types
 of neural network systems where the input and output are used in the network.
 A well known training method is the Standard Backpropagation algorithm
 (BP)
\begin_inset CommandInset citation
LatexCommand cite
key "Rumelhart:1986:PDP:104279"

\end_inset

.
 Backpropagation is a method to calculate the gradient of the loss function
 with respect to the weights.
 In this algorithm, the sign of the gradient of a weight indicates whether
 the error varies directly with, or inversely to, the weight.
 The objective function 
\begin_inset Formula $E(W)$
\end_inset

 is calculated after BP applies an update to the weights in the network:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle\omega_{ji}=-\eta\frac{\partial E}{\partial\omega_{ji}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\eta$
\end_inset

 is constant positive value called learning rate.
 The momentum rate 
\begin_inset Formula $\beta$
\end_inset

 can be added to the current weight change, this often speeds up the learning
 process
\begin_inset CommandInset citation
LatexCommand cite
key "Sutskever2013"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle\omega_{ji}^{'}=\beta\triangle\omega_{ji}-\eta\frac{\partial E}{\partial\omega_{ji}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In order to update the weights the online mode, or the average error over
 all training data (batch mode) can be used.
 Besides, more sophisticated alternatives to the BP algorithm, such as the
 Levenberg-Marquardt(LM) have been found faster convergence algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "hess-9-111-2005"

\end_inset

.
 In this algorithm the weight update is obtained by the following equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\varDelta\omega=-\left[H+\mu I\right]^{-1}J^{T}\rho
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\mu$
\end_inset

 is a learning rate, 
\series bold

\begin_inset Formula $J$
\end_inset

 
\series default
the jacobian matrix, which is the first derivatives of the network error
 with respect to the weights and biases, and 
\begin_inset Formula $\rho$
\end_inset

 is a vector of network errors.
 Finally, 
\series bold

\begin_inset Formula $H$
\end_inset

 
\series default
is an approximation of the Hessian matrix.
\end_layout

\begin_layout Subsection
Recurrent Learning
\end_layout

\begin_layout Standard
Standard BP algorithm is not suited for networks with cycles in them.
 Nonetheless, we can apply some artifices and see the RNN like feedforward
 network by unfolding this "RNN" network in time as shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:unfold"

\end_inset

.
 The RNN is interpreted as layered network that keeps the same weights to
 reusing, we assume the time delay of 1 in each connection in order to create
 an equivalent feedforward network 
\begin_inset CommandInset citation
LatexCommand cite
key "Williams90anefficient"

\end_inset

.
 This extension of the BP method is called Backpropagation Through Time(BPTT).
 In BPTT the number of networks copies is equal to time step 
\series bold

\begin_inset Formula $T$
\end_inset


\series default
.
 It would be impractical in the online training since the memory footprint
 grows linearly with the time.
 Therefore, the network unfolding is limited to a chosen truncation depth
 to keep the method feasible 
\begin_inset CommandInset citation
LatexCommand cite
key "hess-17-253-2013"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/RNNunfoldprocess-ES.pdf
	lyxscale 30
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:unfold"

\end_inset

Recurrent Neural Network unfolded in time, the hidden units grouped at time
 
\begin_inset Formula $T$
\end_inset

 get inputs from other neurons at previous time steps such as 
\begin_inset Formula $T-1,T-2,\ldots T$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
More sophisticated methods were developed to overcome BPTT limitations,
 for example Real-Time Recurrent Learning
\begin_inset CommandInset citation
LatexCommand cite
key "Williams:1989:LAC:1351124.1351135"

\end_inset

, Clockwork Recurrent Network (CW-RNN) that splits hidden layer into M modules
 running at different clocks
\begin_inset CommandInset citation
LatexCommand cite
key "Kuna2015"

\end_inset

 and the extended Kalman Filter (EKF) method, which each time estimates
 optimal weights, given a series of observed outputs, for more details see
\begin_inset CommandInset citation
LatexCommand cite
key "Sum:1998:EKF:296468.296482"

\end_inset

.
 However, these methods suffer shortcomings related to the modeling complexity
 and optimization(gradient)
\begin_inset CommandInset citation
LatexCommand cite
key "LukoseviciusJaeger09"

\end_inset

.
 This means that many updates may be necessary and it could be computationally
 expensive.
 The gradient information might becomes useless
\begin_inset CommandInset citation
LatexCommand cite
key "doya1992bifurcations"

\end_inset

.
\end_layout

\begin_layout Subsection
Echo state network
\end_layout

\begin_layout Standard
Echo state networks (ESN) establish an efficient and powerful approach to
 recurrent neural network (RNN) training.
 Unlike the traditional RNN, such as Elman networks, that are organized
 in layers and contain feedback connections.
 The core part of ESN is a single reservoir consisting of a mass of neurons
 that are randomly interconnected and self-connected.
 The reservoir itself remains unchanged, once it is selected.
 The efficient learning can be achieved by determining the weights of the
 connections between the reservoir and the output layer.
 ESN overcome the slow convergence, high computational requirements and
 suboptimal estimates of the model parameters.
 All these are founded in training algorithms based on direct optimization
 of the network weights.
 
\end_layout

\begin_layout Standard
It may seem surprising that an ANN with random connections may be effective,
 but random parameters have been successful in several domains.
 For example, random projections have been used in mechanical learning and
 dimensionality reduction 
\begin_inset CommandInset citation
LatexCommand cite
key "Datar:2004:LHS:997817.997857"

\end_inset

, and more recently, random weights have been shown to be effective for
 convolutional neuronal networks in problems with training data.
 very limited 
\begin_inset CommandInset citation
LatexCommand cite
key "Jarret,Saxe_551"

\end_inset

.
 Therefore, it should not be surprising that random connections are effective
 at least in some situations.
\end_layout

\begin_layout Standard
Although ESN does not solve the problem of training RNN entirely, its impressive
 performance suggests that an initialization based on ESN could be successful.
 This is confirmed by the results of 
\begin_inset CommandInset citation
LatexCommand citep
key "Ilya"

\end_inset

 in his work.
\end_layout

\begin_layout Standard
Now we will proceed to give the formal description of the ESN network.
\end_layout

\begin_layout Subsection
Training Echo State Networks
\end_layout

\begin_layout Standard
The RC paradigm creates a random ANN that remains unchanged throughout the
 training.
 This ANN is called 
\begin_inset Quotes eld
\end_inset

Reservoir
\begin_inset Quotes erd
\end_inset

, which is passively excited by the input signal and maintains a non-linear
 transformation of the input history in its state.
 
\end_layout

\begin_layout Standard
The main equation of ESN, where we do not use any input, but only the output
 feedback, is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x(t+1)=f(W\text{·}x(t)+W^{fb}\text{·}y(t))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Or alternatively, with entries:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x(t+1)=f(W^{in}\text{·}u(t)+W\text{·}x(t)+W^{fb}\text{·}y(t))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x(t)$
\end_inset

 is the vector that contains all the states of the reservoir at time 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $W$
\end_inset

 is the reservoir matrix, where each input 
\begin_inset Formula $W_{ij}$
\end_inset

 corresponds to the connection between the neuron 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $W^{fb}$
\end_inset

 is the matrix of the feedback vector, and 
\begin_inset Formula $y(t)$
\end_inset

 is the output at time 
\begin_inset Formula $t$
\end_inset

.
 In the second version of the equation we see 
\begin_inset Formula $u(t)$
\end_inset

 multiplied by the input vector 
\begin_inset Formula $W^{in}$
\end_inset

.
 This equation represents the initial phase of the network, where the output
 actually works as input, driving the dynamics of the network.
 The function 
\begin_inset Formula $f$
\end_inset

 is generally chosen to be the hyperbolic tangent for the internal neurons
 (
\begin_inset Formula $tanh$
\end_inset

) and the identity function for the output neuron.
 To account for echo state property, the internal weight matrix 
\begin_inset Formula $W$
\end_inset

 is typically scaled as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
W\leftarrow\frac{\alpha}{\mid\lambda_{max}\mid}W
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mid\lambda_{max}\mid$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is the spectral radius of 
\begin_inset Formula $W$
\end_inset

, and 
\begin_inset Formula $\alpha\in(0,1)$
\end_inset

 is a scaling parameter.
 Additionally, due to the influence of initial reservoir states, a certain
 number of initial steps need to be abandoned during the training, called
 washout phase
\begin_inset CommandInset citation
LatexCommand cite
key "SUN201717"

\end_inset

.
\end_layout

\begin_layout Standard
The algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:training_ESN"

\end_inset

 summarizes how the training of an ESN network is carried out.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
caption{training ESN}
\end_layout

\begin_layout Plain Layout


\backslash
State {$i 
\backslash
gets historicalSequence_{size}$}
\end_layout

\begin_layout Plain Layout


\backslash
State {$j 
\backslash
gets reservoir_{size}$}
\end_layout

\begin_layout Plain Layout


\backslash
State {$M 
\backslash
gets array(i,j)$}  
\backslash
Comment{state matrix} 
\end_layout

\begin_layout Plain Layout


\backslash
State {$Forgetpoints 
\backslash
gets Z$} 
\backslash
Comment{forget initial steps} 
\end_layout

\begin_layout Plain Layout


\backslash
While{$t 
\backslash
le samples_{size}$}
\end_layout

\begin_layout Plain Layout


\backslash
If{$t 
\backslash
le Forgetpoints$}
\end_layout

\begin_layout Plain Layout


\backslash
State continue;
\end_layout

\begin_layout Plain Layout


\backslash
Else
\end_layout

\begin_layout Plain Layout


\backslash
State {$M(t,:)
\backslash
gets x(t)$}
\end_layout

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:training_ESN"

\end_inset

Standard training algorithm of ESN network, the states are collected in
 an 
\begin_inset Formula $M$
\end_inset

 matrix that has in each row the status vector 
\begin_inset Formula $x(t)$
\end_inset

 and in each column the neurons of the reservoir.
 Therefore, 
\begin_inset Formula $M$
\end_inset

 is a matrix of examples (rows) by the reservoir dimension (columns).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Echo State Network learning
\end_layout

\begin_layout Standard
The linear output layer of an ESN network is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y\left(n\right)=W_{out}\left[1;u\left(n\right);x\left(n\right)\right]\label{eq:readoutequation-1-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where, 
\begin_inset Formula $y\left(n\right)\in R^{N_{y}}$
\end_inset

, is the output vector with dimension 
\begin_inset Formula $N_{y}$
\end_inset

 of the network, 
\begin_inset Formula $W_{out}\in R^{N_{y}\times\left(1+N_{u}+N_{x}\right)}$
\end_inset

, is the output weights matrix and 
\begin_inset Formula $\left[.;.;.\right]$
\end_inset

 means a vertical vector concatenation (or matrix).
 Now in order to get the matrix 
\begin_inset Formula $W_{out}$
\end_inset

, we can use linear algebra procedures such as Pseudo-Inverse or Ridge regressio
n 
\begin_inset CommandInset citation
LatexCommand cite
key "Jaeger2001a"

\end_inset

.
\end_layout

\begin_layout Subsection
Moore-Penrose pseudoinverse
\end_layout

\begin_layout Standard
The Pseudoinverse, or Moore-Penrose Pseudoinverse, is a generalization of
 a inverse matrix, but for matrices that are not rectangular.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
W_{out}=pinv(M)∗T\label{eq:pinv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where, 
\begin_inset Formula $W_{out}$
\end_inset

, is the vector of output weights, and 
\begin_inset Formula $T$
\end_inset

, is the vector of expected values.
 Therefore, we have a set of 
\begin_inset Formula $m$
\end_inset

 equations with 
\begin_inset Formula $n$
\end_inset

 unknowns, where 
\begin_inset Formula $n$
\end_inset

 is the number of neurons, the size and the inputs of 
\begin_inset Formula $W_{out}$
\end_inset

 are the respective weights of the states of the neurons.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $A$
\end_inset

 be  
\begin_inset Formula $(m\times n)$
\end_inset

 matrix, then the Moore-Penrose is unique, denote 
\begin_inset Formula $A*$
\end_inset

, have the size 
\begin_inset Formula $n\times m$
\end_inset

 and satisfy the following four conditions:
\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
AA*A=A
\]

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
A*AA*=A*
\]

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
\left(A*A\right)^{T}=A*A
\]

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
\left(AA*\right)^{T}=AA*
\]

\end_inset


\end_layout

\begin_layout Subsection
Ridge Regression
\end_layout

\begin_layout Standard
Finding the optimal weights, which minimize the squared error between 
\begin_inset Formula $y\left(n\right)$
\end_inset

 and 
\begin_inset Formula $y^{target}\left(n\right)$
\end_inset

, is equivalent to solving a system of linear equations typically overdetermined.
 The system is overdetermined, because typically 
\begin_inset Formula $T\gg1+N_{u}+N_{x}$
\end_inset

 .
\end_layout

\begin_layout Standard
There are well-known standard ways of solving the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinv"

\end_inset

, probably the most universal and stable solution in this context is the
 Ridge regression, also known as regression with regularization from Tikhonov:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
W_{out}=Y^{target}X^{T}\left(XX^{T}+\beta I\right)^{-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where, 
\begin_inset Formula $β$
\end_inset

 is a regularization coefficient, and 
\begin_inset Formula $I$
\end_inset

 is the identity matrix.
\end_layout

\begin_layout Standard
We show only two of the methods that can be used to solve the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinv"

\end_inset

, although the latter is not very trivial it is preferred to use.
 Next, the output values ​​of the network will be validated, using the adjusted
 matrix 
\begin_inset Formula $W_{out}$
\end_inset

.
\end_layout

\begin_layout Subsection
Echo State Network Validation
\end_layout

\begin_layout Standard
In this stage, the network is executed on the test data, where the states
 of the neurons at time 
\begin_inset Formula $t=0$
\end_inset

 in the validation phase are states of the neurons at time 
\begin_inset Formula $t=m$
\end_inset

 in the learning phase.
 The difference now is that the output is calculated by the network using
 the weights of 
\begin_inset Formula $W_{out}$
\end_inset

, so these values ​​are not previously known.
 The equations for the validation phase are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\widehat{y}(t)=f^{out}\left(x\left(t\right)*W^{out}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x\left(t+1\right)=f\left(W\cdot x\left(t\right)+W^{fb}\cdot\widehat{y}\left(t\right)\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where, 
\begin_inset Formula $\widehat{y}$
\end_inset

 is the output after the pseudoinverse calculation.
 It is common to use an identity output function, however in the equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinv"

\end_inset

, some non-linear transformation can be applied, such as 
\begin_inset Formula $tanh$
\end_inset

.
 Also when calculating weights (
\begin_inset Formula $W_{out}$
\end_inset

) we could use a non-linear technique, such as perceptron, SVM, or Ridge
 regression.
 Finally, to evaluate the ESN network, we usually calculate the Normalized
 Mean Square Error (NRMSE) which is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
NRMSE=\sqrt{\frac{\parallel\widehat{y}-y\parallel{}^{2}}{m*\sigma_{y}^{2}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where, 
\begin_inset Formula $\sigma_{y}^{2}$
\end_inset

  is the variance of the desired output 
\begin_inset Formula $y,m$
\end_inset

 is the validation sequence, and, is the expected output, 
\begin_inset Formula $\widehat{y}$
\end_inset

 is the output calculated by the ESN network after the process Learning.
\end_layout

\begin_layout Section
Stochastic Streamflow model ESN
\end_layout

\begin_layout Standard
In the section, we present a new stochastic model architecture for time
 series prediction, SSMESN, and give the details of the whole architecture
 and learning algorithm.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Y_{v,t}=f\left(R_{v,t}+E_{v,t}\right)\label{eq:eq_general}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The model has the purpose of generating scenarios of hydrological synthetic
 data, in terms of monthly intervals, for this an architecture based on
 Recurrent Neural Networks (RNAR) was used as the deterministic component
 where:
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $Y_{v,t}$
\end_inset

, are the synthetic values ​​produced by the model,
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $E_{v,t}$
\end_inset

, are the values ​​produced by the RNAR,
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $R_{v,t}$
\end_inset

, is the stochastic component represented by the equations ([eq: randomCom]).
\end_layout

\begin_layout Standard
• The function 
\begin_inset Formula $f$
\end_inset

 represents the inverse of the preprocessing transformations.
\end_layout

\begin_layout Standard
So that our model can synthesize hydrological monthly time series (periodic,
 stationary), you have to adjust the parameters not only in time intervals
 of the series, but also in your period.
 For example, if the period is monthly, our model will be composed of 12
 stochastic components.
 In this case, the model is formed by a chain of its components, between
 the input value to the RNAR and the next period, as can be seen in the
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Esquema-del-proceso"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/eschema_model.pdf
	lyxscale 50
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Esquema-del-proceso"

\end_inset

Proposed stochastic process, the celestial and black spheres represent the
 stochastic and deterministic components, respectively, a chaining between
 the value of the time series of a period that is part of the input to the
 Recurrent Network of the next period.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Stochastic component
\end_layout

\begin_layout Standard
In terms of statistics, the variable 
\begin_inset Formula $R_{v,t}$
\end_inset

 represents a normally distributed random noise, which takes into account
 the uncertainty that usually affects hydrological processes.
 
\begin_inset Formula $R_{v,t}$
\end_inset

 is added to provide variability in 
\begin_inset Formula $Y_{v,t}$
\end_inset

 which remains even after 
\begin_inset Formula $Y_{v,t-1}$
\end_inset

 is known 
\begin_inset CommandInset citation
LatexCommand cite
key "loucks2005water"

\end_inset

.
 Each 
\begin_inset Formula $R_{v,t}$
\end_inset

 is independent of past values 
\begin_inset Formula $​​Y_{v,w}$
\end_inset

, where 
\begin_inset Formula $w\leq t-1$
\end_inset

, and 
\begin_inset Formula $R_{v,t}$
\end_inset

 is independent of 
\begin_inset Formula $R_{v,w}$
\end_inset

 for 
\begin_inset Formula $w\neq t-1$
\end_inset

 .
 This component is the same stochastic part from Thomas & Fiering model,
 
\end_layout

\begin_layout Subsection
First order stationary Markov model or Thomas Fiering model (Stationary)
\end_layout

\begin_layout Standard
The method consists of the use of twelve linear regression equations: 
\end_layout

\begin_layout Standard
\align center
\begin_inset Formula 
\begin{equation}
\begin{aligned}Y_{jan}= & \overline{Y}_{jan}+b_{jan}\left(Y_{dec}-\overline{Y}_{dec}\right)+\varepsilon_{jan}\\
Y_{feb}= & \overline{Y}_{feb}+b_{feb}\left(Y_{jan}-\overline{Y}_{jan}\right)+\varepsilon_{feb}\\
...= & ...
\end{aligned}
\label{eq:thomas2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
From equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:thomas2"

\end_inset

, the regression analysis of 
\begin_inset Formula $Y_{t+1}$
\end_inset

 is given in 
\begin_inset Formula $Y_{t}$
\end_inset

 over years where 
\begin_inset Formula $t=1(january),2(february),...,12(december)$
\end_inset

, 
\begin_inset Formula $b_{j}$
\end_inset

 is the regression coefficient between month 
\begin_inset Formula $t+1$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 The monthly regression line can be determined from previous values, by
 means of the general equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\widehat{Y}_{t+1}=\overline{Y}_{t+1}+b_{t}\left(Y_{t}-\overline{Y}_{t}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The variability of these points plotted from the regression line that reflect
 the variance on this line is added by the additional component 
\begin_inset Formula $R_{t}$
\end_inset

 (in red in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Random"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
R_{t+1}=\epsilon\times\sigma_{t+1}\times\sqrt{\left(1-r_{t}^{2}\right)}\label{eq:randomCom}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where:
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $\sigma_{t+1}$
\end_inset

, is the standard deviation in month 
\begin_inset Formula $t+1$
\end_inset

.
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $r_{t}$
\end_inset

, is the correlation coefficient between months 
\begin_inset Formula $t+1$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 (throughout the historical record).
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $\epsilon=N(0,1)$
\end_inset

, a normally distributed random noise with zero mean and standard deviation
 one.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/thomas_fiering_random_es.pdf
	lyxscale 30
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Random"

\end_inset

Conditional distribution of 
\begin_inset Formula $Y_{t+1}$
\end_inset

 given 
\begin_inset Formula $Y_{t}=y_{t}$
\end_inset

 for two normal random variables.
 The red oval represents the stochastic component used by our model in its
 final form.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The procedure for using the stochastic component 
\begin_inset Formula $R_{t}$
\end_inset

 in our model is described in the follow pseudocode 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:ramdom_component"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
caption{Random Component $R_{t}$}
\end_layout

\begin_layout Plain Layout


\backslash
State {$
\backslash
textit{n} 
\backslash
gets 
\backslash
text{length of }
\backslash
textit{(Historical record)}$}
\end_layout

\begin_layout Plain Layout


\backslash
For{$t= 0 
\backslash
to 11 $}
\backslash
Comment{monthly, t:=0,january} 
\end_layout

\begin_layout Plain Layout


\backslash
State $v
\backslash
gets t$
\end_layout

\begin_layout Plain Layout


\backslash
State $j
\backslash
gets 1$
\end_layout

\begin_layout Plain Layout


\backslash
State $sum_{t}
\backslash
gets 0, sum_{t+1}
\backslash
gets 0$
\end_layout

\begin_layout Plain Layout


\backslash
While{$v 
\backslash
le n$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $sum_{t}
\backslash
gets sum_{t}+Y_{t,v}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $sum_{t+1}
\backslash
gets sum_{t+1}+Y_{t+1,v}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $j
\backslash
gets j+1, v
\backslash
gets v+12*j$
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
overline{Y}_{t}=
\backslash
frac{sum_{t}}{n}$}
\backslash
Comment{streamflow }
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
overline{Y}_{t+1}=
\backslash
frac{sum_{t+1}}{n}$}
\backslash
Comment{streamflow}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $j
\backslash
gets 1, v
\backslash
gets t$
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout


\backslash
While{$v 
\backslash
le n$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $temp1_{t}
\backslash
gets temp1_{t} + 
\backslash
left(Y_{t,v}-
\backslash
overline{Y}_{t}
\backslash
right)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $temp2_{t}
\backslash
gets temp2_{t} + 
\backslash
left(Y_{t,v}-
\backslash
overline{Y}_{t}
\backslash
right)^{2}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $temp1_{t+1}
\backslash
gets temp1_{t+1} + 
\backslash
left(Y_{t+1,v}-
\backslash
overline{Y}_{t+1}
\backslash
right)$	
\end_layout

\begin_layout Plain Layout

	
\backslash
State $temp2_{t+1}
\backslash
gets temp2_{t+1} + 
\backslash
left(Y_{t+1,v}-
\backslash
overline{Y}_{t+1}
\backslash
right)^{2}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $j
\backslash
gets j+1, v
\backslash
gets v+12*j$
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
sigma_{t}=
\backslash
sqrt{
\backslash
frac{
\backslash
left(temp2_{t}
\backslash
right)}{n-1}}$}
\backslash
Comment{standar deviation} 
\end_layout

\begin_layout Plain Layout


\backslash
State{$r_{t}=
\backslash
frac{
\backslash
left(temp1_{t}
\backslash
right)
\backslash
times 
\backslash
left(temp1_{t+1}
\backslash
right)}{
\backslash
sqrt{
\backslash
left(temp2_{t}
\backslash
right) 
\backslash
times 
\backslash
left(temp2_{t+1}
\backslash
right)}}$}
\backslash
Comment{correlation coefficient between $t$ and $t+1$} 
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
epsilon=N(0,1)$} 
\backslash
Comment{normally distributed random noise}
\end_layout

\begin_layout Plain Layout


\backslash
State{$R_{t+1}=
\backslash
epsilon
\backslash
times
\backslash
sigma_{t+1}
\backslash
times
\backslash
sqrt{
\backslash
left(1-r_{t}^{2}
\backslash
right)}$}
\backslash
Comment{Stochastic component, The model is then a set of twelve regression
 equations}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:ramdom_component"

\end_inset

Calculate Random Component
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally to generate synthetic time series the procedure is repeated, generating
 a sequence of random numbers 
\begin_inset Formula $\left\{ \epsilon_{1},\epsilon_{2},...\right\} $
\end_inset

  that are replaced in the model, In this work, a recurrent topology  was
 used as a deterministic component.
\end_layout

\begin_layout Subsection
Echo State Component
\end_layout

\begin_layout Standard
This sequential information is preserved in the internal states or processing
 units (PU) of the Recurrent ANN, and allows to manage values at time 
\begin_inset Formula $t$
\end_inset

 without the need for pre-processing or time delay as can be seen in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Generación-de-escenarios"

\end_inset

.
 
\end_layout

\begin_layout Standard
In order to obtain a time series value at time 
\begin_inset Formula $t$
\end_inset

, in our model the Recurrent ANN receives as input the values at time 
\begin_inset Formula $t-1$
\end_inset

.
 The activation of the internal PU (echo states) is updated according to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x\left(t+1\right)=\vartheta\left(W^{in}y_{t+1}+\theta_{t+1}+Wx(t)\right)\label{eq:statesRNN}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where:
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $x(t)$
\end_inset

, is the vector of internal states or PU.
\end_layout

\begin_layout Standard
• W, is the synaptic weights matrix with recurrent connection.
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $y_{t+1}$
\end_inset

, is the input signal, in month 
\begin_inset Formula $t+1$
\end_inset

.
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $W^{in}$
\end_inset

, is the synaptic weight matrix between the input signal and the PU.
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $\vartheta$
\end_inset

, represents the activation function of the internal states (usually a hyperboli
c tangent function).
\end_layout

\begin_layout Standard
• 
\begin_inset Formula $\theta_{t+1}$
\end_inset

, bias.
\end_layout

\begin_layout Standard
The output of the Recurrent ANN is calculated according to the equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E_{t+1}=\delta\left(W^{out}\left(x(t+1)+y_{t}\right)+\theta_{t}\right)\label{eq:ouputRNN}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where: 
\begin_inset Formula $W^{out}$
\end_inset

, is the weight matrix between the internal states 
\begin_inset Formula $x(t+1)$
\end_inset

 added to the input signals 
\begin_inset Formula $y_{t}$
\end_inset

 and the output neurons.
 
\begin_inset Formula $\delta$
\end_inset

 is the activation function of the output neurons, this is a standard tool
 for condensing very small or very large values within a logistic space.
\end_layout

\begin_layout Standard
As can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Esquema-del-proceso"

\end_inset

 the synthetic values 
\begin_inset Formula $Y_{t}$
\end_inset

 is given by the sum of the output of our Recurrent ANN (
\begin_inset Formula $E_{t}$
\end_inset

), and the stochastic part of the Thomas & Fiering model (
\begin_inset Formula $R_{t}$
\end_inset

), described by the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:eq_general"

\end_inset

.
 In order to obtain a mathematical description, the equations are concatenated
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:statesRNN"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ouputRNN"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:eq_general"

\end_inset

, to obtain the following extended equation of our model:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
\begin{aligned}Y_{t+1}= & f\left(\delta\left(W^{out}\times\left(\vartheta\left[W^{in}y_{t}+\theta_{t}+Wx(t-1)\right]+y_{t}\right)+\theta_{t}\right)+R_{t}\right)\end{aligned}
\label{eq:final_form}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/propose_model.pdf
	lyxscale 20
	scale 4.7

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Generación-de-escenarios"

\end_inset

Generation of synthetic scenarios, the new SSNESN model is observed in detail
 in this work.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sequential information generated by the new model is preserved in the
 internal states of the Recurrent ANN (
\begin_inset Formula $x(t)$
\end_inset

) that allow many steps in time to affect the processing of each new entry.
 Therefore, it is not necessary to incorporate "sliding window" techniques
 
\begin_inset CommandInset citation
LatexCommand citep
key "Vafaeipour2014"

\end_inset

 in our model, in contrast with similar works in the literature 
\begin_inset CommandInset citation
LatexCommand citep
key "lcdcampos,Awchi"

\end_inset

.
 Often the use of Neural Networks involves the task of estimating a large
 number of hyper-parameters, related to their structure and performance.
 It is evident that to generate synthetic series it is necessary to adjust
 our model with the historical time series.
 We can formalize this problem as: 
\end_layout

\begin_layout Standard
Given a time series 
\begin_inset Formula $Y_{1},Y_{2},...,Y_{t}$
\end_inset

 each in a real space 
\begin_inset Formula $N_{Y-dimensional}$
\end_inset

, the goal is to calculate a learning machine 
\begin_inset Formula $\varphi\left(\bullet,p\right)$
\end_inset

 with 
\begin_inset Formula $p$
\end_inset

 parameters such that it is able to predict (best possible) the value of
 any observation of 
\begin_inset Formula $Y_{t+\tau}\left(\tau\geq1\right)$
\end_inset

.
 The objective function associated with 
\begin_inset Formula $\varphi\left(\bullet,p\right)$
\end_inset

 for a single tuple 
\begin_inset Formula $\left(\varphi\left(Y_{t},p\right),Y_{t+\tau}\right)$
\end_inset

, is defined by a distance function that measures the deviation between
 the target 
\begin_inset Formula $Y_{t+\tau}$
\end_inset

 and the prediction 
\begin_inset Formula $\varphi\left(Y_{t},p\right)$
\end_inset

.
 In this work, the standardized Mean Square Error distance is used for an
 arbitrary range of time 
\begin_inset Formula $\tau$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
NRMSE=\sqrt{\frac{1}{\sigma\left(Y_{\tau}\right)\tau}\sum_{t=1}^{\tau}\sum_{i=1}^{N_{Y}}\left(\varphi_{i}\left(Y_{t},p\right)-Y_{i,\left(t+\tau\right)}\right)^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
ESN training and design settings
\end_layout

\begin_layout Standard
In this work the configuration of ESN basically depends on the following
 parameters: the size of the reservoir (internal states), the spectral radius
 of the reservoir matrix, the density of the reservoir matrix and the topology
 of the reservoir network (connectivity) 
\begin_inset CommandInset citation
LatexCommand citep
key "LukoseviciusJaeger09,Baserrech2015"

\end_inset

.
 We decided to explain in more detail the three most common learning approaches
 in the ESN literature
\end_layout

\begin_layout Subsubsection
Reservoir Size
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand citet
key "Jaeger2001a"

\end_inset

, an obviously crucial parameter of the model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:eq_general"

\end_inset

 is 
\begin_inset Formula $N_{x}$
\end_inset

, the number of units in the reservoir.
 The larger the space of the reservoir signals 
\begin_inset Formula $x(t)$
\end_inset

, the easier it will be to find a linear combination of the signals to approxima
te 
\begin_inset Formula $y^{objective}(t)$
\end_inset

.
\end_layout

\begin_layout Standard
Since the training and execution of an ESN is computationally cheap compared
 to other Recurrent ANN approaches, reservoir sizes of order 
\begin_inset Formula $10^{4}$
\end_inset

  are common 
\begin_inset CommandInset citation
LatexCommand citep
key "Alomar:2016:FSE:2934357.2934372"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Spectral Radius
\end_layout

\begin_layout Standard
Basically, the spectral radius controls the stability and memory capacity
 of the model.
 It is denoted by 
\begin_inset Formula $\rho\left(W\right)$
\end_inset

.
 If 
\begin_inset Formula $\rho\left(W\right)$
\end_inset

 is less than 1, the stability of the ESN can be assured 
\begin_inset CommandInset citation
LatexCommand citep
key "LukoseviciusJaeger09"

\end_inset

.
 To satisfy this condition, the 
\begin_inset Formula $W$
\end_inset

 matrix is usually scaled as follows: 
\begin_inset Formula $W\leftarrow\left(\frac{\alpha}{\rho\left(W\right)}\right)W$
\end_inset

, where 
\begin_inset Formula $\alpha$
\end_inset

 is a constant in 
\begin_inset Formula $<0.1]$
\end_inset

.
 A spectral radius
\begin_inset Formula $\rho\left(W\right)$
\end_inset

 close to 1 is appropriate for learning tasks that require long times.
 On the other hand, a value of 
\begin_inset Formula $\rho\left(W\right)$
\end_inset

 close to 0 is suitable for tasks that require short memory 
\begin_inset CommandInset citation
LatexCommand citep
key "Verstraeten2007391"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Connectivity
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand citet
key "Song:2010:ECS:1801020.1801314"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Jaeger78"

\end_inset

, connectivity is defined as the number of non-zero weights of the total
 number of weights in the network.
 In the non-linear case, that is, when an activation function 
\begin_inset Formula $tanh$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:statesRNN"

\end_inset

 is used, some researchers have reported no effect of the connectivity value
 
\begin_inset CommandInset citation
LatexCommand cite
key "Millea2014,Koryakin201235"

\end_inset

.
\end_layout

\begin_layout Standard
The reservoir topology can also impact the performance of the model.
 Frequently, weights are initialized using the uniform distribution in an
 arbitrary range.
 Several approaches have been studied to find better reservoir weights than
 random weights.
 Approaches that use topographic maps, optimization of swarms can be seen
 in 
\begin_inset CommandInset citation
LatexCommand citep
key "BasterrechAlba,BasterrechFyfe"

\end_inset

.
 But, the most common approach continues to use random initialization 
\begin_inset CommandInset citation
LatexCommand citep
key "Baserrech2015"

\end_inset

.
 
\end_layout

\begin_layout Standard
Finally, with respect to connectivity, it should be noted that we use non-linear
 ESNs (
\begin_inset Formula $tanh$
\end_inset

 as a function of activation).
 Based on the studies of 
\begin_inset CommandInset citation
LatexCommand citet
key "Jaeger78"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Millea2014"

\end_inset

, in practice, around 
\begin_inset Formula $20%\ensuremath{\%}
$
\end_inset

 % of non-zero values are often used 
\begin_inset CommandInset citation
LatexCommand citep
key "LukoseviciusJaeger09"

\end_inset

, to define the reservoir matrices.
 Therefore, based on these preliminary study, we considered the random initializ
ation of the reservoir and connectivity of 
\begin_inset Formula $20%\ensuremath{\%}
$
\end_inset

 % as standard approach to initialize the weights of our Echo State Component.
\end_layout

\begin_layout Standard
In order to find a good ESN configuration, we used a very simple computational
 method (Random Optimization), in which the connectivity is not considered,
 only the size of the reservoir and the spectral radius.
\end_layout

\begin_layout Section
Results and discussion
\end_layout

\begin_layout Section
Hidrometereological Data Generation Using ESN-RNN With Random Component
\begin_inset CommandInset label
LatexCommand label
name "sec:Hidrometereological-data-generat"

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references/refsjournal"
options "elsarticle-num-names"

\end_inset


\end_layout

\end_body
\end_document
